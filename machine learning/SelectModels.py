# -*- coding: utf-8 -*-
"""cs6242 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q6hEKghEIj-7w_9yRDHri3zdghJMs5II
"""

# Import numpy, pandas, matpltlib.pyplot, sklearn modules and seaborn
import numpy as np
import pandas as pd
import io
import re
import os
from scipy.stats import boxcox

# Import DecisionTreeClassifier from sklearn.tree
from sklearn.tree import DecisionTreeClassifier

# Import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# Import LogisticRegression
from sklearn.linear_model import LogisticRegression

#Import Others
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve, auc

#Load Data
df = pd.read_csv('df_v2.csv')

#Drop unname column
df.drop(df.filter(regex="Unname"),axis=1, inplace=True)

df.info()

df["Severity"].value_counts()

#Resampling
df_bl = pd.concat([df[df['Severity']==4].sample(100000, replace = True, random_state=42),
                   df[df['Severity']==2].sample(50000, random_state=42),
                   df[df['Severity']==3].sample(5000, random_state=42),
                   df[df['Severity']==1].sample(1500, replace = True, random_state=42)], axis=0)

df_bl["Severity"].value_counts()

# Generate dummies for categorical data
cat = ['Side','State','Timezone','Wind_Direction', 'Weekday', 'Month', 'Hour','Sunrise_Sunset']
df_bl[cat] = df_bl[cat].astype('category')
df_bl = pd.get_dummies(df_bl, columns=cat, drop_first=True)
df_bl.head()

# List of classification algorithms
algo_lst=['Logistic Regression','Decision Trees','Random Forest', 
          'Support Vector Machine', 'Neural network', 'XGBoost' ]
accuracy_lst=[0.639, 0.876, 0.907, 0.708, 0.639, 0.908]
f1_lst=[0.498, 0.866, 0.894, 0.664, 0.498, 0.901]

# Initialize an empty list for the accuracy score for each algorithm
#accuracy_lst=[]

# Initialize an empty list for the f1 score for each algorithm
#f1_lst=[]

#df = df_dummy
X = df_bl.drop('Severity',axis=1)
y = df_bl['Severity']


# split train test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(\
              X, y, test_size=0.20, random_state=42, stratify=y)

#----------------Logistic regression
from sklearn.linear_model import LogisticRegression
import time
tic2 = time.time()
lr = LogisticRegression(random_state=0)
lr.fit(X_train,y_train)
y_pred=lr.predict(X_test)

# Get the accuracy score
acc=accuracy_score(y_test, y_pred)

# Get the f1 score
f1_lr=f1_score(y_test, y_pred, average='weighted')

# Append to the accuracy list
accuracy_lst.append(acc)
f1_lst.append(f1_lr)

# Display results
print("[Logistic regression algorithm] accuracy_score: {:.3f}.".format(acc))
print("[Logistic regression algorithm] f1_score: {:.3f}.".format(f1_lr))
toc2 = time.time()
print('Elapsed time for Losigtic regression is %f seconds \n' % float(toc2 - tic2))

#------------------Decision tree algorithm

# 1. Instantiate dt_entropy, set 'entropy' as the information criterion
dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)

# 1. Fit dt_entropy to the training set
dt_entropy.fit(X_train, y_train)

# 1. Use dt_entropy to predict test set labels
y_pred= dt_entropy.predict(X_test)

# 1. Evaluate accuracy_entropy
accuracy_entropy = accuracy_score(y_test, y_pred)

# 1. Get the f1 score
f1_dt_entropy=f1_score(y_test, y_pred, average='weighted')

# Append to the accuracy list
#accuracy_lst.append(acc)
#f1_lst.append(f1_lr)

# 1. Print accuracy_entropy
print('[Decision Tree -- entropy] accuracy_score: {:.3f}.'.format(accuracy_entropy))
print("[Decision Tree -- entropy] f1_score: {:.3f}.".format(f1_dt_entropy))

tic3 = time.time()

# 2. Instantiate dt_gini, set 'gini' as the information criterion
dt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)

# 2. Fit dt_entropy to the training set
dt_gini.fit(X_train, y_train)

# 2. Use dt_entropy to predict test set labels
y_pred= dt_gini.predict(X_test)

# 2. Evaluate accuracy_entropy
accuracy_gini = accuracy_score(y_test, y_pred)

# 2. Get the f1 score
f1_dt_gini=f1_score(y_test, y_pred, average='weighted')

# 2. Append to the accuracy list
acc=accuracy_gini
accuracy_lst.append(acc)
f1_lst.append(f1_dt_gini)

# 2. Print accuracy_gini
print('[Decision Tree -- gini] accuracy_score: {:.3f}.'.format(accuracy_gini))
print("[Decision Tree -- gini] f1_score: {:.3f}.".format(f1_dt_gini))

toc3 = time.time()
print('Elapsed time for Decision tree is %f seconds \n' % float(toc3 - tic3))

# --------------Random Forest algorithm
tic4 = time.time()
#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=256)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)

# Get the accuracy score
acc=accuracy_score(y_test, y_pred)

# Get the f1 score
f1_clf=f1_score(y_test, y_pred, average='weighted')

# Append to the accuracy list
accuracy_lst.append(acc)
f1_lst.append(f1_clf)

# Model Accuracy, how often is the classifier correct?
print("[Randon forest algorithm] accuracy_score: {:.3f}.".format(acc))
print("[Randon forest algorithm] f1_score: {:.3f}.".format(f1_clf))
toc4 = time.time()
print('Elapsed time for Randon forest is %f seconds \n' % float(toc4 - tic4))

# Commented out IPython magic to ensure Python compatibility.
#Algorithm Random Forest
#Visualize important features
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
pd.set_option('display.max_rows', 350)
pd.set_option('display.max_columns', 350)
plt.style.use('ggplot')

feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)

# Creating a bar plot, displaying only the top k features
k=20
sns.barplot(x=feature_imp[:20], y=feature_imp.index[:k])
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()

# List top k important features
k=20
feature_imp.sort_values(ascending=False)[:k]

#Algorithm Random Forest
#Select the top important features, set the threshold
# Create a selector object that will use the random forest classifier to identify
# features that have an importance of more than 0.03
sfm = SelectFromModel(clf, threshold=0.03)

# Train the selector
sfm.fit(X_train, y_train)

feat_labels=X.columns

# Print the names of the most important features
for feature_list_index in sfm.get_support(indices=True):
    print(feat_labels[feature_list_index])

# Transform the data to create a new dataset containing only the most important features
# Note: We have to apply the transform to both the training X and test X data.
X_important_train = sfm.transform(X_train)
X_important_test = sfm.transform(X_test)

# Create a new random forest classifier for the most important features
clf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)

# Train the new classifier on the new dataset containing the most important features
clf_important.fit(X_important_train, y_train)

# Apply The Full Featured Classifier To The Test Data
y_pred = clf.predict(X_test)

# View The Accuracy Of Our Full Feature Model
print('[Randon forest algorithm -- Full feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))

# Apply The Full Featured Classifier To The Test Data
y_important_pred = clf_important.predict(X_important_test)

# View The Accuracy Of Our Limited Feature Model
print('[Randon forest algorithm -- Limited feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_important_pred)))

#View The F1 Score of Our Full Feature Model
print('[Randon forest algorithm -- Full feature] f1_score: {:.3f}.'.format(f1_score(y_test, y_pred, average='weighted')))

# View The F1 Score Of Our Limited Feature Model
print('[Randon forest algorithm -- Limited feature] f1_score: {:.3f}.'.format(f1_score(y_test, y_important_pred, average='weighted')))

#--------------- Support vector machine algorithm
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, normalize
tic5 = time.time()

scaler = StandardScaler()
scaler.fit(X_train)
scaled_x_train = scaler.transform(X_train)
scaled_x_test = scaler.transform(X_test)


svm = SVC(gamma='auto')
svm.fit(scaled_x_train, y_train)
y_pred = svm.predict(scaled_x_test)

# Get the accuracy score
acc=accuracy_score(y_test, y_pred)

# Get f1-score
f1_svm=f1_score(y_test, y_pred, average='weighted')

# Append to the accuracy list
accuracy_lst.append(acc)
f1_lst.append(f1_svm)

print("[Support vector machine algorithm] accuracy_score: {:.3f}.".format(acc))
print("[Support vector machine algorithm] f1_score: {:.3f}.".format(f1_svm))
toc5 = time.time()
print('Elapsed time for Support vector machine is %f seconds \n' % float(toc5 - tic5))

#--------------- Neural network algorithm
from sklearn import neural_network
import time

tic6 = time.time()
nn = neural_network.MLPClassifier(hidden_layer_sizes=(5, 2))
nn.fit(X_train,y_train)
y_pred=nn.predict(X_test)

# Get the accuracy score
acc=accuracy_score(y_test, y_pred)

# f1_score
f1_nn=f1_score(y_test, y_pred, average='weighted')

# Append to the accuracy list
#accuracy_lst.append(acc)
#f1_lst.append(f1_nn)

print("[Neural network algorithm] accuracy_score: {:.3f}.".format(acc))
print("[Neural network algorithm] f1_score: {:.3f}.".format(f1_nn))
toc6 = time.time()
print('Elapsed time for Neural network is %f seconds \n' % float(toc6 - tic6))

#--------------- XGBoost algorithm
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
tic7 = time.time()
xgb = XGBClassifier(objective= 'multi:softmax',num_class=4,n_fold=4,
                    colsample_bytree = 1,
                    learning_rate = 0.15,
                    max_depth = 5,
                    n_estimators = 600,
                    subsample = 0.3)
xgb.fit(X_train, y_train)

y_pred=xgb.predict(X_test)

# Get the accuracy score
acc=accuracy_score(y_test, y_pred)

# Get f1 score
f1_xgb=f1_score(y_test, y_pred, average='weighted')

# Append to the accuracy list
#accuracy_lst.append(acc)
#f1_lst.append(f1_xgb)


print("[XGBoost algorithm] accuracy_score: {:.3f}.".format(acc))
print("[XGBoost algorithm] f1_score: {:.3f}.".format(f1_xgb))
toc7 = time.time()
print('Elapsed time for Losigtic regression is %f seconds \n' % float(toc7 - tic7))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Make a plot of the accuracy scores for different algorithms

# Generate a list of ticks for y-axis
y_ticks=np.arange(len(algo_lst))

# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score
df_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)

# Export to a file
df_acc.to_csv('./Accuracy_scores_algorithms.csv',index=False)

# Make a plot
ax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')

# Add the data label on to the plot
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),3)), fontsize=10)


# Set the limit, lables, ticks and title
plt.xlim(0,1.1)
plt.xlabel('Accuracy Score')
plt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)
plt.title('Which algorithm is better regarding accuracy score?')

plt.show()

# Make a plot of the accuracy scores for different algorithms

# Generate a list of ticks for y-axis
y_ticks=np.arange(len(algo_lst))

# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score
df_acc=pd.DataFrame(list(zip(algo_lst, f1_lst)), columns=['Algorithm','F1_Score']).sort_values(by=['F1_Score'],ascending = True)

# Export to a file
df_acc.to_csv('./f1_scores_algorithms.csv',index=False)

# Make a plot
ax=df_acc.plot.barh('Algorithm', 'F1_Score', align='center',legend=False,color='0.5')

# Add the data label on to the plot
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),3)), fontsize=10)


# Set the limit, lables, ticks and title
plt.xlim(0,1.1)
plt.xlabel('F1 Score')
plt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)
plt.title('Which algorithm is better regarding f1 score?')

plt.show()